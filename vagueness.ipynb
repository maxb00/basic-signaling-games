{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Vagueness\n",
    "Can we create the conditions for a signaling system to learn to send the null signal in border states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imageio.v2 as imageio\n",
    "from IPython.display import HTML\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_dist(n: int) -> float:\n",
    "    return np.exp(- (n**2) / (4 / np.log(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop(epochs, rewards, threshold=0.95):\n",
    "    return np.sum(rewards[-epochs:]) / epochs > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(filename_base, epochs, seed, fps=10):\n",
    "    images = []\n",
    "    for filename in [f'images/game_{i}.png' for i in range(epochs) if i % 25 == 0]:\n",
    "        images.append(imageio.imread(filename))\n",
    "    if not os.path.exists(f'gifs/{seed}'):\n",
    "        os.mkdir(f'gifs/{seed}')\n",
    "    imageio.mimsave(f'gifs/{seed}/{filename_base}.gif', images, fps=fps)\n",
    "    display(HTML('<img src=\"{}\">'.format(f'gifs/{seed}/{filename_base}.gif')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block is optional, but will make numpy warnings throw like errors that can be caught. I have used this mainly for debugging unexpected instances of infinity or NaN in Sender or Reciever weight calcultation, as `np.exp(309)` or `1e309` both overflow to infinity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "np.seterr(all='warn')\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Setup\n",
    "Initially inspired by https://tomekkorbak.com/2019/10/08/lewis-signaling-games/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self, n_states: int, \n",
    "                 n_signals: int, n_actions: int, \n",
    "                 reward_mod=(1,1), seed: int = 0) -> None:\n",
    "        self.setup = (n_signals, n_actions)\n",
    "        self.pos, self.neg = reward_mod\n",
    "        self.positive, self.negative = reward_mod\n",
    "        self.n_states = n_states\n",
    "        self.state = 0\n",
    "        self.random = np.random.RandomState(seed)\n",
    "\n",
    "    def get_state(self) -> int:\n",
    "        self.state = self.random.randint(self.n_states)\n",
    "        return self.state\n",
    "\n",
    "    def evaluate(self, action: int) -> int:\n",
    "        step = self.n_states / self.setup[0]\n",
    "        correct = self.state // step\n",
    "        return self.pos if action == correct else self.neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender:\n",
    "    def __init__(self, n_stimuli: int, n_signals: int, q_not: float = 1e-6) -> None:\n",
    "        # n_stimuli: number of possible states in the world,\n",
    "        #            each corresponding to a stimulus\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial signal propensity values. Final value of null signal.\n",
    "        self.n_signals = n_signals + 1      # +1 here represents null signal.\n",
    "        self.signal_weights = np.zeros((self.n_signals, n_stimuli))\n",
    "        self.signal_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_signal(self, stimulus: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.signal_weights[:, stimulus])\n",
    "        den = np.sum(np.exp(self.signal_weights[:, stimulus]))\n",
    "        probabilities = num / den\n",
    "        signal = np.random.choice(self.n_signals, p=probabilities)\n",
    "        if signal == self.n_signals-1:\n",
    "            # null action\n",
    "            return -1\n",
    "        self.last_situation = (stimulus, signal)\n",
    "        return signal\n",
    "\n",
    "    def update(self, reward: int) -> None:\n",
    "        # I am capping weight values at 308 due to overflow errors.\n",
    "        stimulus, signal = self.last_situation\n",
    "        self.signal_weights[signal, stimulus] += reward\n",
    "\n",
    "        # after updating the first weight, we must reinforce the surrouding weights\n",
    "        # using a gaussian distribution with a height of 1 and a width of 2\n",
    "        # so that stimulus+2 and stimulus-2 are updated with 1/2 the reward.\n",
    "        for i in range(1, 4):\n",
    "            r = reward * reward_dist(i)\n",
    "\n",
    "            # reward right\n",
    "            if stimulus + i < self.signal_weights.shape[1]:\n",
    "                q_last = self.signal_weights[signal, stimulus + i]\n",
    "                self.signal_weights[signal, stimulus +\n",
    "                                    i] = min(q_last + r, 308)\n",
    "\n",
    "            # reward left\n",
    "            if stimulus - i >= 0:\n",
    "                q_last = self.signal_weights[signal, stimulus - i]\n",
    "                self.signal_weights[signal, stimulus -\n",
    "                                    i] = min(q_last + r, 308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receiver:\n",
    "    def __init__(self, n_signals, n_actions, q_not: float = 1e-6) -> None:\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # n_actions: number of actions that can be taken in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial action propensity value\n",
    "        self.n_actions = n_actions\n",
    "        self.action_weights = np.zeros((n_signals, n_actions))\n",
    "        self.action_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_action(self, signal: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.action_weights[signal, :])\n",
    "        den = np.sum(np.exp(self.action_weights[signal, :]))\n",
    "        probabilities = num / den\n",
    "        action = np.random.choice(self.n_actions, p=probabilities)\n",
    "        self.last_situation = (signal, action)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self, reward: int) -> None:\n",
    "        signal, action = self.last_situation\n",
    "        q_last = self.action_weights[signal, action]\n",
    "        self.action_weights[signal, action] = min(q_last + reward, 308)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "positive_reward = 0.01   # \"Full reward\" for a success in some state. Modifier.\n",
    "negative_reward = -0.03  # \"Full reward\" for a failure in some state. Modifier.\n",
    "epochs = 20_000           # Number of epochs to train for\n",
    "seed = 0                 # Random Number Generator seed. numpy algorithm.\n",
    "world_states = 20        # number of world states. evenly split among signals\n",
    "signals = 2              # number of signals sender can send (not including null)\n",
    "actions = 2              # number of actions reciever can respond with\n",
    "initial_q = 25           # initial propensities. final null action score.\n",
    "gif_fps = 10             # frames per second for gif\n",
    "\n",
    "rew = (positive_reward, negative_reward)\n",
    "# world states should be evenly divisible by action and signals\n",
    "assert world_states % signals == world_states % actions == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize world\n",
    "S = Sender(world_states, signals, initial_q)\n",
    "R = Receiver(signals, actions, initial_q)\n",
    "W = World(world_states, signals, actions, rew, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, last 100 epochs reward: 1.000000e-04\n",
      "Epoch 100, last 100 epochs reward: -1.080000e-02\n",
      "Epoch 200, last 100 epochs reward: -3.000000e-03\n",
      "Epoch 300, last 100 epochs reward: -8.900000e-03\n",
      "Epoch 400, last 100 epochs reward: -4.200000e-03\n",
      "Epoch 500, last 100 epochs reward: -5.200000e-03\n",
      "Epoch 600, last 100 epochs reward: -7.500000e-03\n",
      "Epoch 700, last 100 epochs reward: -2.500000e-03\n",
      "Epoch 800, last 100 epochs reward: -2.800000e-03\n",
      "Epoch 900, last 100 epochs reward: -4.500000e-03\n",
      "Epoch 1000, last 100 epochs reward: -5.600000e-03\n",
      "Epoch 1100, last 100 epochs reward: -5.200000e-03\n",
      "Epoch 1200, last 100 epochs reward: -5.600000e-03\n",
      "Epoch 1300, last 100 epochs reward: -5.400000e-03\n",
      "Epoch 1400, last 100 epochs reward: -6.500000e-03\n",
      "Epoch 1500, last 100 epochs reward: -2.500000e-03\n",
      "Epoch 1600, last 100 epochs reward: -4.200000e-03\n",
      "Epoch 1700, last 100 epochs reward: -3.000000e-03\n",
      "Epoch 1800, last 100 epochs reward: -4.300000e-03\n",
      "Epoch 1900, last 100 epochs reward: -6.900000e-03\n",
      "Epoch 2000, last 100 epochs reward: -4.700000e-03\n",
      "Epoch 2100, last 100 epochs reward: -4.300000e-03\n",
      "Epoch 2200, last 100 epochs reward: -2.700000e-03\n",
      "Epoch 2300, last 100 epochs reward: -4.100000e-03\n",
      "Epoch 2400, last 100 epochs reward: -3.600000e-03\n",
      "Epoch 2500, last 100 epochs reward: -3.100000e-03\n",
      "Epoch 2600, last 100 epochs reward: -2.500000e-03\n",
      "Epoch 2700, last 100 epochs reward: -5.700000e-03\n",
      "Epoch 2800, last 100 epochs reward: -3.700000e-03\n",
      "Epoch 2900, last 100 epochs reward: -5.200000e-03\n",
      "Epoch 3000, last 100 epochs reward: -5.200000e-03\n",
      "Epoch 3100, last 100 epochs reward: -4.600000e-03\n",
      "Epoch 3200, last 100 epochs reward: -3.000000e-03\n",
      "Epoch 3300, last 100 epochs reward: -3.400000e-03\n",
      "Epoch 3400, last 100 epochs reward: -3.800000e-03\n",
      "Epoch 3500, last 100 epochs reward: -4.700000e-03\n",
      "Epoch 3600, last 100 epochs reward: -1.600000e-03\n",
      "Epoch 3700, last 100 epochs reward: -4.000000e-03\n",
      "Epoch 3800, last 100 epochs reward: -4.500000e-03\n",
      "Epoch 3900, last 100 epochs reward: -3.700000e-03\n",
      "Epoch 4000, last 100 epochs reward: -3.100000e-03\n",
      "Epoch 4100, last 100 epochs reward: -2.200000e-03\n",
      "Epoch 4200, last 100 epochs reward: -7.000000e-04\n",
      "Epoch 4300, last 100 epochs reward: -5.400000e-03\n",
      "Epoch 4400, last 100 epochs reward: -2.300000e-03\n",
      "Epoch 4500, last 100 epochs reward: -4.500000e-03\n",
      "Epoch 4600, last 100 epochs reward: -2.100000e-03\n",
      "Epoch 4700, last 100 epochs reward: -1.400000e-03\n",
      "Epoch 4800, last 100 epochs reward: -2.500000e-03\n",
      "Epoch 4900, last 100 epochs reward: -1.700000e-03\n",
      "Epoch 5000, last 100 epochs reward: -3.800000e-03\n",
      "Epoch 5100, last 100 epochs reward: -2.700000e-03\n",
      "Epoch 5200, last 100 epochs reward: -1.900000e-03\n",
      "Epoch 5300, last 100 epochs reward: -1.200000e-03\n",
      "Epoch 5400, last 100 epochs reward: -1.800000e-03\n",
      "Epoch 5500, last 100 epochs reward: -1.100000e-03\n",
      "Epoch 5600, last 100 epochs reward: -3.000000e-03\n",
      "Epoch 5700, last 100 epochs reward: -1.900000e-03\n",
      "Epoch 5800, last 100 epochs reward: -2.000000e-03\n",
      "Epoch 5900, last 100 epochs reward: -1.900000e-03\n",
      "Epoch 6000, last 100 epochs reward: -5.000000e-04\n",
      "Epoch 6100, last 100 epochs reward: -1.200000e-03\n",
      "Epoch 6200, last 100 epochs reward: -1.000000e-03\n",
      "Epoch 6300, last 100 epochs reward: 5.000000e-04\n",
      "Epoch 6400, last 100 epochs reward: -1.100000e-03\n",
      "Epoch 6500, last 100 epochs reward: -2.900000e-03\n",
      "Epoch 6600, last 100 epochs reward: -9.000000e-04\n",
      "Epoch 6700, last 100 epochs reward: 5.000000e-04\n",
      "Epoch 6800, last 100 epochs reward: -2.600000e-03\n",
      "Epoch 6900, last 100 epochs reward: -1.300000e-03\n",
      "Epoch 7000, last 100 epochs reward: -1.600000e-03\n",
      "Epoch 7100, last 100 epochs reward: -9.000000e-04\n",
      "Epoch 7200, last 100 epochs reward: -2.200000e-03\n",
      "Epoch 7300, last 100 epochs reward: -1.200000e-03\n",
      "Epoch 7400, last 100 epochs reward: -3.000000e-04\n",
      "Epoch 7500, last 100 epochs reward: -7.000000e-04\n",
      "Epoch 7600, last 100 epochs reward: -2.600000e-03\n",
      "Epoch 7700, last 100 epochs reward: -1.100000e-03\n",
      "Epoch 7800, last 100 epochs reward: -1.900000e-03\n",
      "Epoch 7900, last 100 epochs reward: 1.000000e-04\n",
      "Epoch 8000, last 100 epochs reward: 6.000000e-04\n",
      "Epoch 8100, last 100 epochs reward: -1.700000e-03\n",
      "Epoch 8200, last 100 epochs reward: -2.100000e-03\n",
      "Epoch 8300, last 100 epochs reward: -9.000000e-04\n",
      "Epoch 8400, last 100 epochs reward: -3.000000e-04\n",
      "Epoch 8500, last 100 epochs reward: -8.000000e-04\n",
      "Epoch 8600, last 100 epochs reward: 3.000000e-04\n",
      "Epoch 8700, last 100 epochs reward: -4.000000e-04\n",
      "Epoch 8800, last 100 epochs reward: -1.000000e-04\n",
      "Epoch 8900, last 100 epochs reward: -7.000000e-04\n",
      "Epoch 9000, last 100 epochs reward: -3.000000e-04\n",
      "Epoch 9100, last 100 epochs reward: -2.000000e-04\n",
      "Epoch 9200, last 100 epochs reward: -1.000000e-04\n",
      "Epoch 9300, last 100 epochs reward: -1.500000e-03\n",
      "Epoch 9400, last 100 epochs reward: -4.000000e-04\n",
      "Epoch 9500, last 100 epochs reward: -1.000000e-03\n",
      "Epoch 9600, last 100 epochs reward: -5.000000e-04\n",
      "Epoch 9700, last 100 epochs reward: -9.000000e-04\n",
      "Epoch 9800, last 100 epochs reward: 5.000000e-04\n",
      "Epoch 9900, last 100 epochs reward: -2.000000e-04\n",
      "Epoch 10000, last 100 epochs reward: -2.200000e-03\n",
      "Epoch 10100, last 100 epochs reward: 1.200000e-03\n",
      "Epoch 10200, last 100 epochs reward: 7.000000e-04\n",
      "Epoch 10300, last 100 epochs reward: 1.000000e-04\n",
      "Epoch 10400, last 100 epochs reward: -8.000000e-04\n",
      "Epoch 10500, last 100 epochs reward: -1.000000e-03\n",
      "Epoch 10600, last 100 epochs reward: 6.000000e-04\n",
      "Epoch 10700, last 100 epochs reward: 1.000000e-03\n",
      "Epoch 10800, last 100 epochs reward: 4.000000e-04\n",
      "Epoch 10900, last 100 epochs reward: -3.000000e-04\n",
      "Epoch 11000, last 100 epochs reward: 9.000000e-04\n",
      "Epoch 11100, last 100 epochs reward: -1.000000e-04\n",
      "Epoch 11200, last 100 epochs reward: 9.000000e-04\n",
      "Epoch 11300, last 100 epochs reward: 5.000000e-04\n",
      "Epoch 11400, last 100 epochs reward: 4.000000e-04\n",
      "Epoch 11500, last 100 epochs reward: 5.000000e-04\n",
      "Epoch 11600, last 100 epochs reward: 1.100000e-03\n",
      "Epoch 11700, last 100 epochs reward: -6.000000e-04\n",
      "Epoch 11800, last 100 epochs reward: 1.800000e-03\n",
      "Epoch 11900, last 100 epochs reward: 1.400000e-03\n",
      "Epoch 12000, last 100 epochs reward: 6.000000e-04\n",
      "Epoch 12100, last 100 epochs reward: 1.000000e-03\n",
      "Epoch 12200, last 100 epochs reward: 1.000000e-03\n",
      "Epoch 12300, last 100 epochs reward: -2.000000e-04\n",
      "Epoch 12400, last 100 epochs reward: 8.000000e-04\n",
      "Epoch 12500, last 100 epochs reward: 1.200000e-03\n",
      "Epoch 12600, last 100 epochs reward: 1.800000e-03\n",
      "Epoch 12700, last 100 epochs reward: 1.800000e-03\n",
      "Epoch 12800, last 100 epochs reward: 2.600000e-03\n",
      "Epoch 12900, last 100 epochs reward: 7.000000e-04\n",
      "Epoch 13000, last 100 epochs reward: 2.400000e-03\n",
      "Epoch 13100, last 100 epochs reward: 2.900000e-03\n",
      "Epoch 13200, last 100 epochs reward: 2.700000e-03\n",
      "Epoch 13300, last 100 epochs reward: 1.500000e-03\n",
      "Epoch 13400, last 100 epochs reward: 1.800000e-03\n",
      "Epoch 13500, last 100 epochs reward: 2.000000e-03\n",
      "Epoch 13600, last 100 epochs reward: 1.700000e-03\n",
      "Epoch 13700, last 100 epochs reward: 1.000000e-03\n",
      "Epoch 13800, last 100 epochs reward: 2.600000e-03\n",
      "Epoch 13900, last 100 epochs reward: 3.400000e-03\n",
      "Epoch 14000, last 100 epochs reward: 2.900000e-03\n",
      "Epoch 14100, last 100 epochs reward: 3.700000e-03\n",
      "Epoch 14200, last 100 epochs reward: 4.100000e-03\n",
      "Epoch 14300, last 100 epochs reward: 4.600000e-03\n",
      "Epoch 14400, last 100 epochs reward: 3.900000e-03\n",
      "Epoch 14500, last 100 epochs reward: 4.800000e-03\n",
      "Epoch 14600, last 100 epochs reward: 5.800000e-03\n",
      "Epoch 14700, last 100 epochs reward: 5.400000e-03\n",
      "Epoch 14800, last 100 epochs reward: 5.300000e-03\n",
      "Epoch 14900, last 100 epochs reward: 5.600000e-03\n",
      "Epoch 15000, last 100 epochs reward: 4.500000e-03\n",
      "Epoch 15100, last 100 epochs reward: 5.200000e-03\n",
      "Epoch 15200, last 100 epochs reward: 5.400000e-03\n",
      "Epoch 15300, last 100 epochs reward: 7.000000e-03\n",
      "Epoch 15400, last 100 epochs reward: 6.800000e-03\n",
      "Epoch 15500, last 100 epochs reward: 5.600000e-03\n",
      "Epoch 15600, last 100 epochs reward: 7.800000e-03\n",
      "Epoch 15700, last 100 epochs reward: 7.200000e-03\n",
      "Epoch 15800, last 100 epochs reward: 7.000000e-03\n",
      "Epoch 15900, last 100 epochs reward: 6.300000e-03\n",
      "Epoch 16000, last 100 epochs reward: 7.300000e-03\n",
      "Epoch 16100, last 100 epochs reward: 7.400000e-03\n",
      "Epoch 16200, last 100 epochs reward: 6.800000e-03\n",
      "Epoch 16300, last 100 epochs reward: 7.800000e-03\n",
      "Epoch 16400, last 100 epochs reward: 7.800000e-03\n",
      "Epoch 16500, last 100 epochs reward: 7.200000e-03\n",
      "Epoch 16600, last 100 epochs reward: 7.700000e-03\n",
      "Epoch 16700, last 100 epochs reward: 7.600000e-03\n",
      "Epoch 16800, last 100 epochs reward: 7.500000e-03\n",
      "Epoch 16900, last 100 epochs reward: 7.500000e-03\n",
      "Epoch 17000, last 100 epochs reward: 9.300000e-03\n",
      "Epoch 17100, last 100 epochs reward: 7.800000e-03\n",
      "Epoch 17200, last 100 epochs reward: 7.700000e-03\n",
      "Epoch 17300, last 100 epochs reward: 8.100000e-03\n",
      "Epoch 17400, last 100 epochs reward: 8.100000e-03\n",
      "Epoch 17500, last 100 epochs reward: 8.100000e-03\n",
      "Epoch 17600, last 100 epochs reward: 8.200000e-03\n",
      "Epoch 17700, last 100 epochs reward: 7.400000e-03\n",
      "Epoch 17800, last 100 epochs reward: 8.600000e-03\n",
      "Epoch 17900, last 100 epochs reward: 9.200000e-03\n",
      "Epoch 18000, last 100 epochs reward: 8.700000e-03\n",
      "Epoch 18100, last 100 epochs reward: 8.700000e-03\n",
      "Epoch 18200, last 100 epochs reward: 8.700000e-03\n",
      "Epoch 18300, last 100 epochs reward: 9.100000e-03\n",
      "Epoch 18400, last 100 epochs reward: 9.100000e-03\n",
      "Epoch 18500, last 100 epochs reward: 9.000000e-03\n",
      "Epoch 18600, last 100 epochs reward: 8.700000e-03\n",
      "Epoch 18700, last 100 epochs reward: 8.400000e-03\n",
      "Epoch 18800, last 100 epochs reward: 8.900000e-03\n",
      "Epoch 18900, last 100 epochs reward: 9.500000e-03\n",
      "Epoch 19000, last 100 epochs reward: 8.400000e-03\n",
      "Epoch 19100, last 100 epochs reward: 9.400000e-03\n",
      "Epoch 19200, last 100 epochs reward: 9.600000e-03\n",
      "Epoch 19300, last 100 epochs reward: 8.900000e-03\n",
      "Epoch 19400, last 100 epochs reward: 9.500000e-03\n",
      "Epoch 19500, last 100 epochs reward: 9.100000e-03\n",
      "Epoch 19600, last 100 epochs reward: 8.800000e-03\n",
      "Epoch 19700, last 100 epochs reward: 9.600000e-03\n",
      "Epoch 19800, last 100 epochs reward: 8.800000e-03\n",
      "Epoch 19900, last 100 epochs reward: 9.100000e-03\n"
     ]
    }
   ],
   "source": [
    "# conduct experiment loop\n",
    "past_rewards = 0\n",
    "history = np.zeros((6, epochs // 25, 3))\n",
    "for epoch in range(epochs):\n",
    "    stimulus = W.get_state()\n",
    "    signal = S.get_signal(stimulus)\n",
    "    if signal != -1:\n",
    "        action = R.get_action(signal)\n",
    "        reward = W.evaluate(action)\n",
    "        past_rewards += reward\n",
    "        S.update(reward)\n",
    "        R.update(reward)\n",
    "    # else null action\n",
    "\n",
    "    if epoch % 25 == 0:\n",
    "        # save history\n",
    "        ep = epoch // 25\n",
    "        history[0, ep] = S.signal_weights[:, 0] # state 0\n",
    "        history[1, ep] = S.signal_weights[:, 5] # state 5\n",
    "        history[2, ep] = S.signal_weights[:, 9] # state 9\n",
    "        history[3, ep] = S.signal_weights[:, 10] # state 10\n",
    "        history[4, ep] = S.signal_weights[:, 14] # state 14\n",
    "        history[5, ep] = S.signal_weights[:, 19] # state 19\n",
    "\n",
    "        # make graphs\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        plt.tight_layout(pad=3)\n",
    "\n",
    "        sns.heatmap(\n",
    "            np.exp(S.signal_weights) /\n",
    "            np.exp(S.signal_weights).sum(axis=0),\n",
    "            square=True, cbar=False, annot=True, fmt='.1f', ax=axs[0])\n",
    "        axs[0].set_ylabel('messages')\n",
    "        axs[0].set_xlabel('world states')\n",
    "        axs[0].set_title(f'Sender\\'s weights')\n",
    "\n",
    "        sns.heatmap(\n",
    "            np.exp(R.action_weights) /\n",
    "            np.exp(R.action_weights).sum(axis=0),\n",
    "            square=True, cbar=False, annot=True, fmt='.2f', ax=axs[1])\n",
    "        axs[1].set_xlabel('actions')\n",
    "        axs[1].set_ylabel('messages')\n",
    "        axs[1].set_title(f'Receiver\\'s weights')\n",
    "        \n",
    "        \n",
    "        fig.suptitle(f'Rollout {epoch}')\n",
    "        plt.savefig(f\"images/game_{epoch}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, last 100 epochs reward: {past_rewards/100:e}')\n",
    "        past_rewards = 0\n",
    "\n",
    "    # TODO: reshape this early stop mechanism\n",
    "    # if early_stop(epochs, past_rewards):\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"gifs/0/20-2-2-game.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_gif(f'{world_states}-{actions}-{signals}-game', epochs, seed, gif_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation to message mapping:\n",
      "[0 0 0 0 0 0 0 0 0 2 1 1 1 1 1 1 1 1 1 1]\n",
      "Message to action mapping:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation to message mapping:\")\n",
    "print(S.signal_weights.argmax(0))\n",
    "print(\"Message to action mapping:\")\n",
    "print(R.action_weights.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(10, 8), sharey=True)\n",
    "\n",
    "ran = range(0, epochs, 25)\n",
    "axs[0, 0].plot(ran, history[0, :, 0], label='action 0')\n",
    "axs[0, 0].plot(ran, history[0, :, 1], label='action 1')\n",
    "axs[0, 0].plot(ran, history[0, :, 2], label='null action')\n",
    "axs[0, 0].set_title('state 0')\n",
    "\n",
    "axs[0, 1].plot(ran, history[1, :, 0], label='action 0')\n",
    "axs[0, 1].plot(ran, history[1, :, 1], label='action 1')\n",
    "axs[0, 1].plot(ran, history[1, :, 2], label='null action')\n",
    "axs[0, 1].set_title('state 5')\n",
    "\n",
    "axs[0, 2].plot(ran, history[2, :, 0], label='action 0')\n",
    "axs[0, 2].plot(ran, history[2, :, 1], label='action 1')\n",
    "axs[0, 2].plot(ran, history[2, :, 2], label='null action')\n",
    "axs[0, 2].set_title('state 9')\n",
    "\n",
    "axs[1, 0].plot(ran, history[3, :, 0], label='action 0')\n",
    "axs[1, 0].plot(ran, history[3, :, 1], label='action 1')\n",
    "axs[1, 0].plot(ran, history[3, :, 2], label='null action')\n",
    "axs[1, 0].set_title('state 10')\n",
    "\n",
    "axs[1, 1].plot(ran, history[4, :, 0], label='action 0')\n",
    "axs[1, 1].plot(ran, history[4, :, 1], label='action 1')\n",
    "axs[1, 1].plot(ran, history[4, :, 2], label='null action')\n",
    "axs[1, 1].set_title('state 14')\n",
    "\n",
    "axs[1, 2].plot(ran, history[5, :, 0], label='action 0')\n",
    "axs[1, 2].plot(ran, history[5, :, 1], label='action 1')\n",
    "axs[1, 2].plot(ran, history[5, :, 2], label='null action')\n",
    "axs[1, 2].set_title('state 19')\n",
    "\n",
    "fig.suptitle(f'Sum action propensities over {epochs} epochs')\n",
    "fig.savefig('images/weights.png')\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd92d645a9d62d833baebbf7f83a9be0e163bc5376810cbaf4f96b1078fea88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
