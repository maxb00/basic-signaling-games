{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Setup: 1 - d / 2\n",
    "What other ways can we model vagueness? Can we make it too hard to learn, or elicit equilibrium?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "import seaborn as sns\n",
    "import os\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "No stimulus generalization. New evaluate fuction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self, n_states: int, \n",
    "                 n_signals: int, n_actions: int, \n",
    "                 seed: int = 0) -> None:\n",
    "        self.setup = (n_signals, n_actions)\n",
    "        self.n_states = n_states\n",
    "        self.state = 0\n",
    "        self.random = np.random.RandomState(seed)\n",
    "\n",
    "    def get_state(self) -> int:\n",
    "        self.state = self.random.randint(self.n_states)\n",
    "        return self.state\n",
    "\n",
    "    def evaluate(self, action: int) -> int:\n",
    "        # 1 minus ((shortest distance between state and action ) / 2)\n",
    "        return 1 - (abs(self.state - action) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender:\n",
    "    def __init__(self, n_stimuli: int, n_signals: int, q_not: float = 1e-6) -> None:\n",
    "        # n_stimuli: number of possible states in the world,\n",
    "        #            each corresponding to a stimulus\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial signal propensity values. Final value of null signal.\n",
    "        self.n_signals = n_signals + 1 # +1 here represents null signal.    \n",
    "        self.signal_weights = np.zeros((self.n_signals, n_stimuli))\n",
    "        self.signal_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_signal(self, stimulus: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.signal_weights[:, stimulus])\n",
    "        den = np.sum(np.exp(self.signal_weights[:, stimulus]))\n",
    "        probabilities = num / den\n",
    "        signal = np.random.choice(self.n_signals, p=probabilities)\n",
    "        if signal == self.n_signals - 1:\n",
    "            return -1\n",
    "        self.last_situation = (stimulus, signal)\n",
    "        return signal\n",
    "\n",
    "    def update(self, reward: int) -> None:\n",
    "        # I am capping weight values at 308 due to overflow errors.\n",
    "        # They must similarly be floored at -323.\n",
    "        stimulus, signal = self.last_situation\n",
    "        q_last = self.signal_weights[signal, stimulus]\n",
    "        self.signal_weights[signal, stimulus] = max(-323, min(q_last + reward, 308))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receiver:\n",
    "    def __init__(self, n_signals, n_actions, q_not: float = 1e-6) -> None:\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # n_actions: number of actions that can be taken in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial action propensity value\n",
    "        self.n_actions = n_actions\n",
    "        self.action_weights = np.zeros((n_signals, n_actions))\n",
    "        self.action_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_action(self, signal: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.action_weights[signal, :])\n",
    "        den = np.sum(np.exp(self.action_weights[signal, :]))\n",
    "        probabilities = num / den\n",
    "        action = np.random.choice(self.n_actions, p=probabilities)\n",
    "        self.last_situation = (signal, action)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self, reward: int) -> None:\n",
    "        signal, action = self.last_situation\n",
    "        q_last = self.action_weights[signal, action]\n",
    "        self.action_weights[signal, action] = max(-323, min(q_last + reward, 308))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, epochs, states, signals, actions):\n",
    "        self.send_hist = np.zeros((epochs // 25, signals+1, states))\n",
    "        self.reci_hist = np.zeros((epochs // 25, signals, actions))\n",
    "        self.epochs = epochs\n",
    "        self.ep = 0\n",
    "\n",
    "    def add(self, send_weights, reci_weights):\n",
    "        self.send_hist[self.ep] = send_weights\n",
    "        self.reci_hist[self.ep] = reci_weights\n",
    "        self.ep += 1\n",
    "\n",
    "    def make_gif(self, fps, filename_base, html=False):\n",
    "        if not os.path.exists(f'./images'):\n",
    "            os.mkdir(f'images') \n",
    "        for i in range(self.ep):\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            plt.tight_layout(pad=3)\n",
    "\n",
    "            sns.heatmap(\n",
    "                np.exp(self.send_hist[i]) /\n",
    "                np.exp(self.send_hist[i]).sum(axis=0),\n",
    "                linewidths=0.5, linecolor='white',\n",
    "                square=True, cbar=False, annot=True, fmt='.1f', ax=axs[0])\n",
    "            axs[0].set_ylabel('messages')\n",
    "            axs[0].set_xlabel('world states')\n",
    "            axs[0].set_title(f'Sender\\'s weights')\n",
    "\n",
    "            sns.heatmap(\n",
    "                np.exp(self.reci_hist[i].T) /\n",
    "                np.exp(self.reci_hist[i].T).sum(axis=0),\n",
    "                linewidths=0.5, linecolor='white',\n",
    "                square=True, cbar=False, annot=True, fmt='.1f', ax=axs[1])\n",
    "            axs[1].set_xlabel('actions')\n",
    "            axs[1].set_ylabel('messages')\n",
    "            axs[1].set_title(f'Receiver\\'s weights')\n",
    "            \n",
    "            \n",
    "            fig.suptitle(f'Rollout {i*25}')\n",
    "            plt.savefig(f\"./images/game_{i*25}.png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        images = []\n",
    "        for filename in [f'./images/game_{j*25}.png' for j in range(self.ep)]:\n",
    "            images.append(imageio.imread(filename))\n",
    "        imageio.mimsave(f'{filename_base}.gif', images, fps=fps)\n",
    "        if html:\n",
    "            display(HTML('<img src=\"{}\">'.format(f'{filename_base}.gif')))\n",
    "        # no return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: 12 states, 2 signals, 12 actions\n",
    "No stimulus generalization, no third bordering state (circularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 12\n",
    "si = 2\n",
    "ac = 12\n",
    "initial = 1\n",
    "epochs = 5000\n",
    "\n",
    "gif_fps = 15\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = World(st, si, ac)\n",
    "S = Sender(st, si, initial)\n",
    "R = Receiver(si, ac, initial)\n",
    "H = History(epochs, st, si, ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow = past_rewards = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    stimulus = W.get_state()\n",
    "    signal = S.get_signal(stimulus)\n",
    "    if signal != -1:\n",
    "        action = R.get_action(signal)\n",
    "        reward = W.evaluate(action)\n",
    "        past_rewards += reward\n",
    "        S.update(reward)\n",
    "        R.update(reward)\n",
    "    # else null action\n",
    "    \n",
    "    if epoch % 25 == 0:\n",
    "        # save history\n",
    "        H.add(S.signal_weights, R.action_weights)\n",
    "        \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        slow = past_rewards / 100\n",
    "        past_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"12-2-12-game-A.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H.make_gif(gif_fps, f'{st}-{si}-{ac}-game-A', html=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Stimulus Generalization\n",
    "Can we still learn anything with stimulus generalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_dist(n: int) -> float:\n",
    "    # Gaussian with FWHM of 2.\n",
    "    return np.exp(- (n**2) / (4 / np.log(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender:\n",
    "    def __init__(self, n_stimuli: int, n_signals: int, q_not: float = 1e-6) -> None:\n",
    "        # n_stimuli: number of possible states in the world,\n",
    "        #            each corresponding to a stimulus\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial signal propensity values. Final value of null signal.\n",
    "        self.n_signals = n_signals + 1 # +1 here represents null signal.    \n",
    "        self.signal_weights = np.zeros((self.n_signals, n_stimuli))\n",
    "        self.signal_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_signal(self, stimulus: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.signal_weights[:, stimulus])\n",
    "        den = np.sum(np.exp(self.signal_weights[:, stimulus]))\n",
    "        probabilities = num / den\n",
    "        signal = np.random.choice(self.n_signals, p=probabilities)\n",
    "        if signal == self.n_signals - 1:\n",
    "            return -1\n",
    "        self.last_situation = (stimulus, signal)\n",
    "        return signal\n",
    "\n",
    "    def update(self, reward: int) -> None:\n",
    "        # I am capping weight values at 308 due to overflow errors.\n",
    "        # They must similarly be floored at -323.\n",
    "        stimulus, signal = self.last_situation\n",
    "        q_last = self.signal_weights[signal, stimulus]\n",
    "        self.signal_weights[signal, stimulus] = max(-323, min(q_last + reward, 308))\n",
    "        \n",
    "        for i in range(1, 4):\n",
    "            r = reward * reward_dist(i)\n",
    "\n",
    "            # reward right\n",
    "            if stimulus + i < self.signal_weights.shape[1]:\n",
    "                q_last = self.signal_weights[signal, stimulus + i]\n",
    "                self.signal_weights[signal, stimulus +\n",
    "                                    i] = max(-323, min(q_last + r, 308))\n",
    "\n",
    "            # reward left\n",
    "            if stimulus - i >= 0:\n",
    "                q_last = self.signal_weights[signal, stimulus - i]\n",
    "                self.signal_weights[signal, stimulus -\n",
    "                                    i] = max(-323, min(q_last + r, 308))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receiver:\n",
    "    def __init__(self, n_signals, n_actions, q_not: float = 1e-6) -> None:\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # n_actions: number of actions that can be taken in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial action propensity value\n",
    "        self.n_actions = n_actions\n",
    "        self.action_weights = np.zeros((n_signals, n_actions))\n",
    "        self.action_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_action(self, signal: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.action_weights[signal, :])\n",
    "        den = np.sum(np.exp(self.action_weights[signal, :]))\n",
    "        probabilities = num / den\n",
    "        action = np.random.choice(self.n_actions, p=probabilities)\n",
    "        self.last_situation = (signal, action)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self, reward: int) -> None:\n",
    "        signal, action = self.last_situation\n",
    "        q_last = self.action_weights[signal, action]\n",
    "        self.action_weights[signal, action] = max(-323, min(q_last + reward, 308))\n",
    "\n",
    "        for i in range(1, 4):\n",
    "            r = reward * reward_dist(i)\n",
    "\n",
    "            # reward right\n",
    "            if stimulus + i < self.action_weights.shape[1]:\n",
    "                q_last = self.action_weights[signal, stimulus + i]\n",
    "                self.action_weights[signal, stimulus +\n",
    "                                    i] = max(-323, min(q_last + r, 308))\n",
    "\n",
    "            # reward left\n",
    "            if stimulus - i >= 0:\n",
    "                q_last = self.action_weights[signal, stimulus - i]\n",
    "                self.action_weights[signal, stimulus -\n",
    "                                    i] = max(-323, min(q_last + r, 308))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 12\n",
    "si = 2\n",
    "ac = 12\n",
    "initial = 1\n",
    "epochs = 5000\n",
    "\n",
    "gif_fps = 15\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = World(st, si, ac)\n",
    "S = Sender(st, si, initial)\n",
    "R = Receiver(si, ac, initial)\n",
    "H = History(epochs, st, si, ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow = past_rewards = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    stimulus = W.get_state()\n",
    "    signal = S.get_signal(stimulus)\n",
    "    if signal != -1:\n",
    "        action = R.get_action(signal)\n",
    "        reward = W.evaluate(action)\n",
    "        past_rewards += reward\n",
    "        S.update(reward)\n",
    "        R.update(reward)\n",
    "    # else null action\n",
    "    \n",
    "    if epoch % 25 == 0:\n",
    "        # save history\n",
    "        H.add(S.signal_weights, R.action_weights)\n",
    "        \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        slow = past_rewards / 100\n",
    "        past_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"12-2-12-game-B.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H.make_gif(gif_fps, f'{st}-{si}-{ac}-game-B', html=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Circularity\n",
    "Minimum experiment size: 3 signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self, n_states: int, \n",
    "                 n_signals: int, n_actions: int, \n",
    "                 seed: int = 0) -> None:\n",
    "        self.setup = (n_signals, n_actions)\n",
    "        self.n_states = n_states\n",
    "        self.state = 0\n",
    "        self.random = np.random.RandomState(seed)\n",
    "\n",
    "    def get_state(self) -> int:\n",
    "        self.state = self.random.randint(self.n_states)\n",
    "        return self.state\n",
    "\n",
    "    def evaluate(self, action: int) -> int:\n",
    "        # this assumes self.n_actions == self.n_states\n",
    "        if self.n_states > 2:\n",
    "            # circle game\n",
    "            left = right = action\n",
    "            for i in range(self.n_states):\n",
    "                # print(i, left, right)\n",
    "                if left == self.state or right == self.state:\n",
    "                    return 1 - (i / 2)\n",
    "                # move left pointer\n",
    "                left -= 1 \n",
    "                if left - 1 < 0:\n",
    "                    # modulo\n",
    "                    left = self.n_states - 1\n",
    "                # move right pointer\n",
    "                right += 1\n",
    "                if right >= self.n_states:\n",
    "                    # modulo\n",
    "                    right = 0\n",
    "        else:\n",
    "            # 1 minus ((shortest distance between state and action ) / 2)\n",
    "            return 1 - (abs(self.state - action) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender:\n",
    "    def __init__(self, n_stimuli: int, n_signals: int, q_not: float = 1e-6) -> None:\n",
    "        # n_stimuli: number of possible states in the world,\n",
    "        #            each corresponding to a stimulus\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial signal propensity values. Final value of null signal.\n",
    "        self.n_signals = n_signals + 1 # +1 here represents null signal.    \n",
    "        self.signal_weights = np.zeros((self.n_signals, n_stimuli))\n",
    "        self.signal_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_signal(self, stimulus: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.signal_weights[:, stimulus])\n",
    "        den = np.sum(np.exp(self.signal_weights[:, stimulus]))\n",
    "        probabilities = num / den\n",
    "        signal = np.random.choice(self.n_signals, p=probabilities)\n",
    "        if signal == self.n_signals - 1:\n",
    "            return -1\n",
    "        self.last_situation = (stimulus, signal)\n",
    "        return signal\n",
    "\n",
    "    def update(self, reward: int) -> None:\n",
    "        # I am capping weight values at 308 due to overflow errors.\n",
    "        # They must similarly be floored at -323.\n",
    "        stimulus, signal = self.last_situation\n",
    "        q_last = self.signal_weights[signal, stimulus]\n",
    "        self.signal_weights[signal, stimulus] = max(-323, min(q_last + reward, 308))\n",
    "        l = r = stimulus\n",
    "        for i in range(1,4):\n",
    "            re = reward * reward_dist(i)\n",
    "            # reward right\n",
    "            r += 1\n",
    "            if r >= self.n_signals:\n",
    "                r = 0\n",
    "            q_last = self.signal_weights[signal, r]\n",
    "            self.signal_weights[signal, r] = max(-323, min(q_last + re, 308))\n",
    "\n",
    "            # reward left\n",
    "            l -= 1\n",
    "            if l < 0:\n",
    "                l = self.n_signals - 1\n",
    "            q_last = self.signal_weights[signal, l]\n",
    "            self.signal_weights[signal, l] = max(-323, min(q_last + re, 308))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receiver:\n",
    "    def __init__(self, n_signals, n_actions, q_not: float = 1e-6) -> None:\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # n_actions: number of actions that can be taken in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial action propensity value\n",
    "        self.n_actions = n_actions\n",
    "        self.action_weights = np.zeros((n_signals, n_actions))\n",
    "        self.action_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_action(self, signal: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.action_weights[signal, :])\n",
    "        den = np.sum(np.exp(self.action_weights[signal, :]))\n",
    "        probabilities = num / den\n",
    "        action = np.random.choice(self.n_actions, p=probabilities)\n",
    "        self.last_situation = (signal, action)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self, reward: int) -> None:\n",
    "        signal, action = self.last_situation\n",
    "        q_last = self.action_weights[signal, action]\n",
    "        self.action_weights[signal, action] = max(-323, min(q_last + reward, 308))\n",
    "\n",
    "        l = r = stimulus\n",
    "        for i in range(1,4):\n",
    "            re = reward * reward_dist(i)\n",
    "            # reward right\n",
    "            r += 1\n",
    "            if r >= self.n_actions:\n",
    "                r = 0\n",
    "            q_last = self.action_weights[signal, r]\n",
    "            self.action_weights[signal, r] = max(-323, min(q_last + re, 308))\n",
    "\n",
    "            # reward left\n",
    "            l -= 1\n",
    "            if l < 0:\n",
    "                l = self.n_actions - 1\n",
    "            q_last = self.action_weights[signal, l]\n",
    "            self.action_weights[signal, l] = max(-323, min(q_last + re, 308))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 18\n",
    "si = 3\n",
    "ac = 18\n",
    "initial = 1\n",
    "epochs = 5000\n",
    "\n",
    "gif_fps = 15\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = World(st, si, ac)\n",
    "S = Sender(st, si, initial)\n",
    "R = Receiver(si, ac, initial)\n",
    "H = History(epochs, st, si, ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# else null action\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m25\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     15\u001b[0m     \u001b[39m# save history\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     H\u001b[39m.\u001b[39;49madd(S\u001b[39m.\u001b[39msignal_weights, R\u001b[39m.\u001b[39maction_weights)\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     20\u001b[0m     slow \u001b[39m=\u001b[39m past_rewards \u001b[39m/\u001b[39m \u001b[39m100\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "slow = past_rewards = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    stimulus = W.get_state()\n",
    "    signal = S.get_signal(stimulus)\n",
    "    if signal != -1:\n",
    "        action = R.get_action(signal)\n",
    "        reward = W.evaluate(action)\n",
    "        past_rewards += reward\n",
    "        S.update(reward)\n",
    "        R.update(reward)\n",
    "    # else null action\n",
    "    \n",
    "    if epoch % 25 == 0:\n",
    "        # save history\n",
    "        H.add(S.signal_weights, R.action_weights)\n",
    "        \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        slow = past_rewards / 100\n",
    "        past_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"18-3-18-game-C.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H.make_gif(gif_fps, f'{st}-{si}-{ac}-game-C', html=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expieriment 4: Circularity with No generalization\n",
    "Looping back around to confirm behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self, n_states: int, \n",
    "                 n_signals: int, n_actions: int, \n",
    "                 seed: int = 0) -> None:\n",
    "        self.setup = (n_signals, n_actions)\n",
    "        self.n_states = n_states\n",
    "        self.state = 0\n",
    "        self.random = np.random.RandomState(seed)\n",
    "\n",
    "    def get_state(self) -> int:\n",
    "        self.state = self.random.randint(self.n_states)\n",
    "        return self.state\n",
    "\n",
    "    def evaluate(self, action: int) -> int:\n",
    "        # this assumes self.n_actions == self.n_states\n",
    "        if self.n_states > 2:\n",
    "            # circle game\n",
    "            left = right = action\n",
    "            for i in range(self.n_states):\n",
    "                # print(i, left, right)\n",
    "                if left == self.state or right == self.state:\n",
    "                    return 1 - (i / 3)\n",
    "                # move left pointer\n",
    "                left -= 1 \n",
    "                if left - 1 < 0:\n",
    "                    # modulo\n",
    "                    left = self.n_states - 1\n",
    "                # move right pointer\n",
    "                right += 1\n",
    "                if right >= self.n_states:\n",
    "                    # modulo\n",
    "                    right = 0\n",
    "        else:\n",
    "            # 1 minus ((shortest distance between state and action ) / 2)\n",
    "            return 1 - (abs(self.state - action) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender:\n",
    "    def __init__(self, n_stimuli: int, n_signals: int, q_not: float = 1e-6) -> None:\n",
    "        # n_stimuli: number of possible states in the world,\n",
    "        #            each corresponding to a stimulus\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial signal propensity values. Final value of null signal.\n",
    "        self.n_signals = n_signals + 1 # +1 here represents null signal.    \n",
    "        self.signal_weights = np.zeros((self.n_signals, n_stimuli))\n",
    "        self.rew_hist = np.zeros_like(self.signal_weights)\n",
    "        self.signal_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_signal(self, stimulus: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.signal_weights[:, stimulus])\n",
    "        den = np.sum(np.exp(self.signal_weights[:, stimulus]))\n",
    "        probabilities = num / den\n",
    "        signal = np.random.choice(self.n_signals, p=probabilities)\n",
    "        if signal == self.n_signals - 1:\n",
    "            return -1\n",
    "        self.last_situation = (stimulus, signal)\n",
    "        return signal\n",
    "\n",
    "    def update(self, reward: int) -> None:\n",
    "        # I am capping weight values at 308 due to overflow errors.\n",
    "        # They must similarly be floored at -323.\n",
    "        stimulus, signal = self.last_situation\n",
    "        q_last = self.signal_weights[signal, stimulus]\n",
    "        self.signal_weights[signal, stimulus] = max(-323, min(q_last + reward, 308))\n",
    "        self.rew_hist[signal, stimulus] += reward\n",
    "        \n",
    "\n",
    "    def checkpoint(self):\n",
    "        self.rew_hist /= 100\n",
    "        sav = np.copy(self.rew_hist)\n",
    "        self.rew_hist = np.zeros_like(self.signal_weights)\n",
    "        return sav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receiver:\n",
    "    def __init__(self, n_signals, n_actions, q_not: float = 1e-6) -> None:\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # n_actions: number of actions that can be taken in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial action propensity value\n",
    "        self.n_actions = n_actions\n",
    "        self.action_weights = np.zeros((n_signals, n_actions))\n",
    "        self.rew_hist = np.zeros_like(self.action_weights)\n",
    "        self.action_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_action(self, signal: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.action_weights[signal, :])\n",
    "        den = np.sum(np.exp(self.action_weights[signal, :]))\n",
    "        probabilities = num / den\n",
    "        action = np.random.choice(self.n_actions, p=probabilities)\n",
    "        self.last_situation = (signal, action)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self, reward: int) -> None:\n",
    "        signal, action = self.last_situation\n",
    "        q_last = self.action_weights[signal, action]\n",
    "        self.action_weights[signal, action] = max(-323, min(q_last + reward, 308))\n",
    "        self.rew_hist[signal, action] += reward\n",
    "\n",
    "    def checkpoint(self):\n",
    "        self.rew_hist /= 100\n",
    "        sav = np.copy(self.rew_hist)\n",
    "        self.rew_hist = np.zeros_like(self.action_weights)\n",
    "        return sav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, epochs, states, signals, actions):\n",
    "        self.send_hist = np.zeros((epochs // 25, signals+1, states))\n",
    "        self.reci_hist = np.zeros((epochs // 25, signals, actions))\n",
    "        self.send_rew = np.zeros_like(self.send_hist)\n",
    "        self.reci_rew = np.zeros_like(self.reci_hist)\n",
    "        self.slows = np.zeros((epochs//100))\n",
    "        self.epochs = epochs\n",
    "        self.ep = 0\n",
    "        self.ep2 = 0\n",
    "\n",
    "    def add_25(self, send_weights, reci_weights):\n",
    "        self.send_hist[self.ep] = send_weights\n",
    "        self.reci_hist[self.ep] = reci_weights\n",
    "        self.ep += 1\n",
    "\n",
    "    def add_100(self, send_rew, reci_rew, slow):\n",
    "        self.send_rew[self.ep2] = send_rew\n",
    "        self.reci_rew[self.ep2] = reci_rew\n",
    "        self.slows[self.ep2] = slow\n",
    "        self.ep2 += 1\n",
    "\n",
    "    def make_gif(self, fps, seed, filename_base, html=False):\n",
    "        if not os.path.exists(f'./images'):\n",
    "            os.mkdir(f'images') \n",
    "        for i in range(self.ep):\n",
    "            fig, axs = plt.subplots(2, 1, figsize=(8, 6))\n",
    "            plt.tight_layout(pad=3)\n",
    "\n",
    "            sns.heatmap(\n",
    "                np.exp(self.send_hist[i]) /\n",
    "                np.exp(self.send_hist[i]).sum(axis=0),\n",
    "                linewidth=0.5, linecolor='white',\n",
    "                square=True, cbar=False, annot=True, fmt='.1f', ax=axs[0])\n",
    "            axs[0].set_ylabel('messages')\n",
    "            axs[0].set_xlabel('world states')\n",
    "            axs[0].set_title(f'Sender\\'s weights')\n",
    "\n",
    "            sns.heatmap(\n",
    "                (np.exp(self.reci_hist[i].T) /\n",
    "                np.exp(self.reci_hist[i].T).sum(axis=0)).T,\n",
    "                linewidth=0.5, linecolor='white',\n",
    "                square=True, cbar=False, annot=True, fmt='.1f', ax=axs[1])\n",
    "            axs[1].set_xlabel('actions')\n",
    "            axs[1].set_ylabel('messages')\n",
    "            axs[1].set_title(f'Receiver\\'s weights')\n",
    "            \n",
    "            \n",
    "            fig.suptitle(f'Rollout {i*25}')\n",
    "            plt.savefig(f\"./images/game_{i*25}.png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        images = []\n",
    "        for filename in [f'./images/game_{j*25}.png' for j in range(self.ep)]:\n",
    "            images.append(imageio.imread(filename))\n",
    "        imageio.mimsave(f'{filename_base}.gif', images, fps=fps)\n",
    "        if html:\n",
    "            display(HTML('<img src=\"{}\">'.format(f'{filename_base}.gif')))\n",
    "        # no return\n",
    "    \n",
    "    def print_send_map(self):\n",
    "        final = (self.send_hist[self.ep-1] == self.send_hist[self.ep-1].max(axis=0)[None, :]).astype(int)\n",
    "        print('a|s', end='')\n",
    "        for i in range(final.shape[1]):\n",
    "            print(f'{i:2}', end=' ')\n",
    "        print()\n",
    "        for i in range(final.shape[0]):\n",
    "            for j in range(-1, final.shape[1]):\n",
    "                if j == -1:\n",
    "                    print(f'{i:2}', end=' ')\n",
    "                else:\n",
    "                    print(f'{final[i, j]:2}', end=' ')\n",
    "            print()\n",
    "\n",
    "    def print_reci_map(self):\n",
    "        final = (H.reci_hist[-1].T == H.reci_hist[-1].T.max(axis=0)[None, :]).astype(int).T\n",
    "        print('m|a', end='')\n",
    "        for i in range(final.shape[1]):\n",
    "            print(f'{i:2}', end=' ')\n",
    "        print()\n",
    "        for i in range(final.shape[0]):\n",
    "            for j in range(-1, final.shape[1]):\n",
    "                if j == -1:\n",
    "                    print(f'{i:2}', end=' ')\n",
    "                else:\n",
    "                    print(f'{final[i, j]:2}', end=' ')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 18\n",
    "si = 3\n",
    "ac = 18\n",
    "initial = 2\n",
    "epochs = 5000\n",
    "\n",
    "gif_fps = 15\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = World(st, si, ac, seed)\n",
    "S = Sender(st, si, initial)\n",
    "R = Receiver(si, ac, initial)\n",
    "H = History(epochs, st, si, ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow = past_rewards = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    stimulus = W.get_state()\n",
    "    signal = S.get_signal(stimulus)\n",
    "    if signal != -1:\n",
    "        action = R.get_action(signal)\n",
    "        reward = W.evaluate(action)\n",
    "        past_rewards += reward\n",
    "        S.update(reward)\n",
    "        R.update(reward)\n",
    "    # else null action\n",
    "    \n",
    "    if epoch % 25 == 0:\n",
    "        # save history\n",
    "        H.add_25(S.signal_weights, R.action_weights)\n",
    "        \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        slow = past_rewards / 100\n",
    "        past_rewards = 0\n",
    "        H.add_100(S.checkpoint(), R.checkpoint(), slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a|s 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 \n",
      " 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1 \n",
      " 1  0  0  0  0  0  0  0  0  1  0  0  1  1  0  0  0  0  0 \n",
      " 2  0  0  0  0  0  0  1  1  0  1  1  0  0  0  0  0  0  0 \n",
      " 3  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0 \n"
     ]
    }
   ],
   "source": [
    "# final action-state mapping\n",
    "H.print_send_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"18-3-18-game-D.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H.make_gif(gif_fps, seed, f'{st}-{si}-{ac}-game-D', html=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd92d645a9d62d833baebbf7f83a9be0e163bc5376810cbaf4f96b1078fea88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
