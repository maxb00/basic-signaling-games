{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vagueness, repeated\n",
    "This notebook contains a task very similar to the one outlines in `vagueness.ipynb`, but with a large-scale generator to test a variety of parameter values systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imageio.v2 as imageio\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import time\n",
    "from itertools import product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards are multiplied by the output of this function\n",
    "def reward_dist(n: int) -> float:\n",
    "    # Gaussian with FWHM of 2.\n",
    "    return np.exp(- (n**2) / (4 / np.log(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self, n_states: int, \n",
    "                 n_signals: int, n_actions: int, \n",
    "                 reward_mod=(1,1), seed: int = 0) -> None:\n",
    "        self.setup = (n_signals, n_actions)\n",
    "        self.pos, self.neg = reward_mod\n",
    "        self.positive, self.negative = reward_mod\n",
    "        self.n_states = n_states\n",
    "        self.state = 0\n",
    "        self.random = np.random.RandomState(seed)\n",
    "\n",
    "    def get_state(self) -> int:\n",
    "        self.state = self.random.randint(self.n_states)\n",
    "        return self.state\n",
    "\n",
    "    def evaluate(self, action: int) -> int:\n",
    "        step = self.n_states / self.setup[0]\n",
    "        correct = self.state // step\n",
    "        return self.pos if action == correct else self.neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender:\n",
    "    def __init__(self, n_stimuli: int, n_signals: int, q_not: float = 1e-6) -> None:\n",
    "        # n_stimuli: number of possible states in the world,\n",
    "        #            each corresponding to a stimulus\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial signal propensity values. Final value of null signal.\n",
    "        self.n_signals = n_signals + 1      # +1 here represents null signal.\n",
    "        self.signal_weights = np.zeros((self.n_signals, n_stimuli))\n",
    "        self.signal_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_signal(self, stimulus: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.signal_weights[:, stimulus])\n",
    "        den = np.sum(np.exp(self.signal_weights[:, stimulus]))\n",
    "        probabilities = num / den\n",
    "        signal = np.random.choice(self.n_signals, p=probabilities)\n",
    "        if signal == self.n_signals-1:\n",
    "            # null action\n",
    "            return -1\n",
    "        self.last_situation = (stimulus, signal)\n",
    "        return signal\n",
    "\n",
    "    def update(self, reward: int) -> None:\n",
    "        # I am capping weight values at 308 due to overflow errors.\n",
    "        stimulus, signal = self.last_situation\n",
    "        self.signal_weights[signal, stimulus] += reward\n",
    "\n",
    "        # after updating the first weight, we must reinforce the surrouding weights\n",
    "        # using a gaussian distribution with a height of 1 and a width of 2\n",
    "        # so that stimulus+2 and stimulus-2 are updated with 1/2 the reward.\n",
    "        for i in range(1, 4):\n",
    "            r = reward * reward_dist(i)\n",
    "\n",
    "            # reward right\n",
    "            if stimulus + i < self.signal_weights.shape[1]:\n",
    "                q_last = self.signal_weights[signal, stimulus + i]\n",
    "                self.signal_weights[signal, stimulus +\n",
    "                                    i] = min(q_last + r, 308)\n",
    "\n",
    "            # reward left\n",
    "            if stimulus - i >= 0:\n",
    "                q_last = self.signal_weights[signal, stimulus - i]\n",
    "                self.signal_weights[signal, stimulus -\n",
    "                                    i] = min(q_last + r, 308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receiver:\n",
    "    def __init__(self, n_signals, n_actions, q_not: float = 1e-6) -> None:\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # n_actions: number of actions that can be taken in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial action propensity value\n",
    "        self.n_actions = n_actions\n",
    "        self.action_weights = np.zeros((n_signals, n_actions))\n",
    "        self.action_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_action(self, signal: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.action_weights[signal, :])\n",
    "        den = np.sum(np.exp(self.action_weights[signal, :]))\n",
    "        probabilities = num / den\n",
    "        action = np.random.choice(self.n_actions, p=probabilities)\n",
    "        self.last_situation = (signal, action)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self, reward: int) -> None:\n",
    "        signal, action = self.last_situation\n",
    "        q_last = self.action_weights[signal, action]\n",
    "        self.action_weights[signal, action] = min(q_last + reward, 308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(filename_base, epochs, seed, fps=10):\n",
    "    images = []\n",
    "    for filename in [f'images/game_{i}.png' for i in range(epochs) if i % 25 == 0]:\n",
    "        images.append(imageio.imread(filename))\n",
    "    if not os.path.exists(f'gifs/{seed}'):\n",
    "        os.mkdir(f'gifs/{seed}')\n",
    "    imageio.mimsave(f'gifs/{seed}/{filename_base}.gif', images, fps=fps)\n",
    "    display(HTML('<img src=\"{}\">'.format(f'gifs/{seed}/{filename_base}.gif')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(epochs, history):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 8), sharey=True)\n",
    "\n",
    "    ran = range(0, epochs, 25)\n",
    "    axs[0, 0].plot(ran, history[0, :, 0], label='action 0')\n",
    "    axs[0, 0].plot(ran, history[0, :, 1], label='action 1')\n",
    "    axs[0, 0].plot(ran, history[0, :, 2], label='null action')\n",
    "    axs[0, 0].set_title('state 0')\n",
    "\n",
    "    axs[0, 1].plot(ran, history[1, :, 0], label='action 0')\n",
    "    axs[0, 1].plot(ran, history[1, :, 1], label='action 1')\n",
    "    axs[0, 1].plot(ran, history[1, :, 2], label='null action')\n",
    "    axs[0, 1].set_title('state 5')\n",
    "\n",
    "    axs[0, 2].plot(ran, history[2, :, 0], label='action 0')\n",
    "    axs[0, 2].plot(ran, history[2, :, 1], label='action 1')\n",
    "    axs[0, 2].plot(ran, history[2, :, 2], label='null action')\n",
    "    axs[0, 2].set_title('state 9')\n",
    "\n",
    "    axs[1, 0].plot(ran, history[3, :, 0], label='action 0')\n",
    "    axs[1, 0].plot(ran, history[3, :, 1], label='action 1')\n",
    "    axs[1, 0].plot(ran, history[3, :, 2], label='null action')\n",
    "    axs[1, 0].set_title('state 10')\n",
    "\n",
    "    axs[1, 1].plot(ran, history[4, :, 0], label='action 0')\n",
    "    axs[1, 1].plot(ran, history[4, :, 1], label='action 1')\n",
    "    axs[1, 1].plot(ran, history[4, :, 2], label='null action')\n",
    "    axs[1, 1].set_title('state 14')\n",
    "\n",
    "    axs[1, 2].plot(ran, history[5, :, 0], label='action 0')\n",
    "    axs[1, 2].plot(ran, history[5, :, 1], label='action 1')\n",
    "    axs[1, 2].plot(ran, history[5, :, 2], label='null action')\n",
    "    axs[1, 2].set_title('state 19')\n",
    "\n",
    "    fig.suptitle(f'Sum action propensities over {epochs} epochs')\n",
    "    fig.savefig('images/weights.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipliers = range(1, 6) # 1, 2, 3, 4, 5\n",
    "negatives = [m*(10**x) for m in multipliers for x in range(-3, 1)]\n",
    "weights = [m*(10**x) for m in multipliers for x in range(-6, 3)]\n",
    "\n",
    "def gener():\n",
    "    for neg in negatives:\n",
    "        for weight in weights:\n",
    "            yield (neg, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "positive = 0.01          # reward for correct action\n",
    "epochs = 20_000          # Number of epochs to train for\n",
    "world_states = 20        # number of world states. evenly split among signals\n",
    "signals = 2              # number of signals sender can send (not including null)\n",
    "actions = 2              # number of actions reciever can respond with\n",
    "gif_fps = 10             # frames per second for gif\n",
    "\n",
    "# world states should be evenly divisible by action and signals\n",
    "assert world_states % signals == world_states % actions == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for negative, weight in product(negatives, weights):\n",
    "    rew = (positive, negative)\n",
    "    seed = int(time.time())\n",
    "    S = Sender(world_states, signals, weight)\n",
    "    R = Receiver(signals, actions, weight)\n",
    "    W = World(world_states, signals, actions, rew, seed)\n",
    "    past_rewards = slow = 0\n",
    "    history = np.zeros((6, epochs // 25, 3))\n",
    "    for epoch in range(epochs):\n",
    "        stimulus = W.get_state()\n",
    "        signal = S.get_signal(stimulus)\n",
    "        if signal != -1:\n",
    "            action = R.get_action(signal)\n",
    "            reward = W.evaluate(action)\n",
    "            past_rewards += reward\n",
    "            S.update(reward)\n",
    "            R.update(reward)\n",
    "        # else null action\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            # save history\n",
    "            ep = epoch // 25\n",
    "            history[0, ep] = S.signal_weights[:, 0] # state 0\n",
    "            history[1, ep] = S.signal_weights[:, 5] # state 5\n",
    "            history[2, ep] = S.signal_weights[:, 9] # state 9\n",
    "            history[3, ep] = S.signal_weights[:, 10] # state 10\n",
    "            history[4, ep] = S.signal_weights[:, 14] # state 14\n",
    "            history[5, ep] = S.signal_weights[:, 19] # state 19\n",
    "\n",
    "            # make graphs\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            plt.tight_layout(pad=3)\n",
    "\n",
    "            sns.heatmap(\n",
    "                np.exp(S.signal_weights) /\n",
    "                np.exp(S.signal_weights).sum(axis=0),\n",
    "                square=True, cbar=False, annot=True, fmt='.1f', ax=axs[0])\n",
    "            axs[0].set_ylabel('messages')\n",
    "            axs[0].set_xlabel('world states')\n",
    "            axs[0].set_title(f'Sender\\'s weights')\n",
    "\n",
    "            sns.heatmap(\n",
    "                np.exp(R.action_weights) /\n",
    "                np.exp(R.action_weights).sum(axis=0),\n",
    "                square=True, cbar=False, annot=True, fmt='.2f', ax=axs[1])\n",
    "            axs[1].set_xlabel('actions')\n",
    "            axs[1].set_ylabel('messages')\n",
    "            axs[1].set_title(f'Receiver\\'s weights')\n",
    "            \n",
    "            \n",
    "            fig.suptitle(f'Rollout {epoch}')\n",
    "            plt.savefig(f\"images/game_{epoch}.png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, last 100 epochs reward: {past_rewards/100:e}')\n",
    "            slow = past_rewards / 100\n",
    "            past_rewards = 0\n",
    "\n",
    "    # now we decide if this is a run to flag\n",
    "    if (np.argmax(S.signal_weights[:, 9]) == 2 or \n",
    "        np.argmax(S.signal_weights[:, 10]) == 2 or\n",
    "        slow < 0):\n",
    "        make_gif(f'{world_states}-{actions}-{signals}-game', epochs, seed, gif_fps)\n",
    "        make_graph(epochs, history)\n",
    "        with open(f'gifs/{seed}/params.txt', 'w') as f:\n",
    "            f.write(f'negative: {negative},\\ninitial weight: {weight}\\nfinal average reward: {slow}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd92d645a9d62d833baebbf7f83a9be0e163bc5376810cbaf4f96b1078fea88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
