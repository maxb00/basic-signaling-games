{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Lewis Signaling Games\n",
    "Employs Roth-Erev (Herrnstein) learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imageio.v2 as imageio\n",
    "from IPython.display import HTML\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "epochs = 10000\n",
    "# seed = np.floor(time()).astype(int)\n",
    "seed = 0\n",
    "states = 3\n",
    "actions = 3\n",
    "signals = 3\n",
    "initial_weights = 1\n",
    "\n",
    "# constants for early stopping\n",
    "stable_epochs = 7\n",
    "threshold = 0.97"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Classes - in the state that produces the most basic signaling game.\n",
    "Initial code influenced heavily by https://tomekkorbak.com/2019/10/08/lewis-signaling-games/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self, n_states: int, seed: int) -> None:\n",
    "        self.n_states = n_states\n",
    "        self.state = 0\n",
    "        self.random = np.random.RandomState(seed)\n",
    "\n",
    "    def get_state(self) -> int:\n",
    "        self.state = self.random.randint(self.n_states)\n",
    "        return self.state\n",
    "\n",
    "    def evaluate(self, action: int) -> int:\n",
    "        return 1 if action == self.state else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender:\n",
    "    def __init__(self, n_stimuli: int, n_signals: int, q_not: float = 1e-6) -> None:\n",
    "        # n_stimuli: number of possible states in the world,\n",
    "        #            each corresponding to a stimulus\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial probabilities of sending each signal before a reward\n",
    "        self.n_signals = n_signals\n",
    "        self.signal_weights = np.zeros((n_stimuli, n_signals))\n",
    "        self.signal_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_signal(self, stimulus: int) -> int:\n",
    "        # p(i) = q(i) / sum(q)\n",
    "        num = self.signal_weights[stimulus, :]\n",
    "        den = np.sum(self.signal_weights[stimulus, :])\n",
    "        probabilities = num / den\n",
    "        signal = np.random.choice(self.n_signals, p=probabilities)\n",
    "        self.last_situation = (stimulus, signal)\n",
    "        return signal\n",
    "\n",
    "    def update(self, reward: int) -> None:\n",
    "        stimulus, signal = self.last_situation\n",
    "        self.signal_weights[stimulus, signal] += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receiver:\n",
    "    def __init__(self, n_signals, n_actions, q_not: float = 1e-6) -> None:\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # n_actions: number of actions that can be taken in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial probabilities of taking each action before a reward\n",
    "        self.n_actions = n_actions\n",
    "        self.action_weights = np.zeros((n_signals, n_actions))\n",
    "        self.action_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_action(self, signal: int) -> int:\n",
    "        # p(i) = q(i) / sum(q)\n",
    "        num = self.action_weights[signal, :]\n",
    "        den = np.sum(self.action_weights[signal, :])\n",
    "        probabilities = num / den\n",
    "        action = np.random.choice(self.n_actions, p=probabilities)\n",
    "        self.last_situation = (signal, action)\n",
    "        return action\n",
    "\n",
    "    def update(self, reward: int) -> None:\n",
    "        signal, action = self.last_situation\n",
    "        self.action_weights[signal, action] += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(filename_base):\n",
    "    images = []\n",
    "    nm = filename_base.split('-')[-1]\n",
    "    for filename in [f'images/{nm}_{i}.png' for i in range(epochs) if i % 25 == 0]:\n",
    "        images.append(imageio.imread(filename))\n",
    "    imageio.mimsave(f'{filename_base}.gif', images, fps=10)\n",
    "    # display(Image(filename=f'{filename_base}.gif'))\n",
    "    display(HTML('<img src=\"{}\">'.format(f'{filename_base}.gif')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop(epochs, rewards, threshold=0.95):\n",
    "    return np.sum(rewards[-epochs:]) / epochs > threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = World(states, seed)\n",
    "S = Sender(states, signals, q_not=initial_weights)\n",
    "R = Receiver(signals, actions, q_not=initial_weights)\n",
    "past_rewards = 0\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, last 100 epochs reward: 0.0\n",
      "Epoch 100, last 100 epochs reward: 0.43\n",
      "Epoch 200, last 100 epochs reward: 0.42\n",
      "Epoch 300, last 100 epochs reward: 0.52\n",
      "Epoch 400, last 100 epochs reward: 0.63\n",
      "Epoch 500, last 100 epochs reward: 0.67\n",
      "Epoch 600, last 100 epochs reward: 0.85\n",
      "Epoch 700, last 100 epochs reward: 0.75\n",
      "Epoch 800, last 100 epochs reward: 0.85\n",
      "Epoch 900, last 100 epochs reward: 0.79\n",
      "Epoch 1000, last 100 epochs reward: 0.87\n",
      "Epoch 1100, last 100 epochs reward: 0.86\n",
      "Epoch 1200, last 100 epochs reward: 0.89\n",
      "Epoch 1300, last 100 epochs reward: 0.9\n",
      "Epoch 1400, last 100 epochs reward: 0.94\n",
      "Epoch 1500, last 100 epochs reward: 0.91\n",
      "Epoch 1600, last 100 epochs reward: 0.9\n",
      "Epoch 1700, last 100 epochs reward: 0.93\n",
      "Epoch 1800, last 100 epochs reward: 0.93\n",
      "Epoch 1900, last 100 epochs reward: 0.94\n",
      "Epoch 2000, last 100 epochs reward: 0.94\n",
      "Epoch 2100, last 100 epochs reward: 0.89\n",
      "Epoch 2200, last 100 epochs reward: 0.97\n",
      "Epoch 2300, last 100 epochs reward: 0.97\n",
      "Epoch 2400, last 100 epochs reward: 0.94\n",
      "Epoch 2500, last 100 epochs reward: 0.96\n",
      "Epoch 2600, last 100 epochs reward: 0.94\n",
      "Epoch 2700, last 100 epochs reward: 0.99\n",
      "Epoch 2800, last 100 epochs reward: 0.96\n",
      "Epoch 2900, last 100 epochs reward: 0.97\n",
      "Epoch 3000, last 100 epochs reward: 0.94\n",
      "Epoch 3100, last 100 epochs reward: 0.95\n",
      "Epoch 3200, last 100 epochs reward: 0.98\n",
      "Epoch 3300, last 100 epochs reward: 0.96\n",
      "Epoch 3400, last 100 epochs reward: 0.94\n",
      "Epoch 3500, last 100 epochs reward: 0.97\n",
      "Epoch 3600, last 100 epochs reward: 0.96\n",
      "Epoch 3700, last 100 epochs reward: 0.99\n",
      "Epoch 3800, last 100 epochs reward: 0.99\n",
      "Epoch 3900, last 100 epochs reward: 0.97\n",
      "Epoch 4000, last 100 epochs reward: 0.98\n",
      "Early stop at epoch 4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    stimulus = W.get_state()\n",
    "    signal = S.get_signal(stimulus)\n",
    "    action = R.get_action(signal)\n",
    "    reward = W.evaluate(action)\n",
    "    past_rewards += reward\n",
    "    S.update(reward)\n",
    "    R.update(reward)\n",
    "\n",
    "    if epoch % 25 == 0:\n",
    "        plt.tight_layout(pad=0)\n",
    "        plot = sns.heatmap(\n",
    "            # np.exp(R.action_weights) /\n",
    "            # np.exp(R.action_weights).sum(axis=0),\n",
    "            R.action_weights / R.action_weights.sum(axis=0),\n",
    "            square=True, cbar=False, annot=True, fmt='.1f'\n",
    "        ).get_figure()\n",
    "        plt.xlabel('messages')\n",
    "        plt.ylabel('actions')\n",
    "        plt.title(f'Receiver\\'s weights, rollout {epoch}')\n",
    "        plt.savefig(f\"images/receiver_{epoch}.png\")\n",
    "        plt.clf()\n",
    "\n",
    "        plot = sns.heatmap(\n",
    "            # np.exp(S.signal_weights) /\n",
    "            # np.exp(S.signal_weights).sum(axis=0),\n",
    "            S.signal_weights / S.signal_weights.sum(axis=0),\n",
    "            square=True, cbar=False, annot=True, fmt='.1f'\n",
    "        ).get_figure()\n",
    "        plt.xlabel('world states')\n",
    "        plt.ylabel('messages')\n",
    "        plt.title(f'Sender\\'s weights, rollout {epoch}')\n",
    "        plt.savefig(f\"images/sender_{epoch}.png\")\n",
    "        plt.clf()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, last 100 epochs reward: {past_rewards/100}')\n",
    "        # print(f\"Last State: {stimulus}, Signal: {signal}, Action: {action}, Reward: {reward}\")\n",
    "        history.append(past_rewards/100)\n",
    "        past_rewards = 0\n",
    "\n",
    "    if early_stop(stable_epochs, history, threshold):\n",
    "        print(f'Early stop at epoch {epoch}')\n",
    "        epochs = epoch\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"3-3-3-sender.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_gif(f'{states}-{actions}-{signals}-sender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"3-3-3-receiver.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_gif(f'{states}-{actions}-{signals}-receiver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation to message mapping:\n",
      "[0 2 1]\n",
      "Message to action mapping:\n",
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation to message mapping:\")\n",
    "print(S.signal_weights.argmax(1))\n",
    "print(\"Message to action mapping:\")\n",
    "print(R.action_weights.argmax(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cc53ca4bcaf722c762c264fce9851c096d2b4ce79674ffe56f966359a856f76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
