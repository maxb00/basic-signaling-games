{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vagueness, repeated\n",
    "This notebook contains a task very similar to the one outlines in `vagueness.ipynb`, but with a large-scale generator to test a variety of parameter values systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imageio.v2 as imageio\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import time\n",
    "from itertools import product\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards are multiplied by the output of this function\n",
    "def reward_dist(n: int) -> float:\n",
    "    # Gaussian with FWHM of 2.\n",
    "    return np.exp(- (n**2) / (4 / np.log(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self, n_states: int, \n",
    "                 n_signals: int, n_actions: int, \n",
    "                 reward_mod=(1,1), seed: int = 0) -> None:\n",
    "        self.setup = (n_signals, n_actions)\n",
    "        self.pos, self.neg = reward_mod\n",
    "        self.positive, self.negative = reward_mod\n",
    "        self.n_states = n_states\n",
    "        self.state = 0\n",
    "        self.random = np.random.RandomState(seed)\n",
    "\n",
    "    def get_state(self) -> int:\n",
    "        self.state = self.random.randint(self.n_states)\n",
    "        return self.state\n",
    "\n",
    "    def evaluate(self, action: int) -> int:\n",
    "        step = self.n_states / self.setup[0]\n",
    "        correct = self.state // step\n",
    "        return self.pos if action == correct else -self.neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender:\n",
    "    def __init__(self, n_stimuli: int, n_signals: int, q_not: float = 1e-6) -> None:\n",
    "        # n_stimuli: number of possible states in the world,\n",
    "        #            each corresponding to a stimulus\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial signal propensity values. Final value of null signal.\n",
    "        self.n_signals = n_signals + 1      # +1 here represents null signal.\n",
    "        self.signal_weights = np.zeros((self.n_signals, n_stimuli))\n",
    "        self.signal_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_signal(self, stimulus: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.signal_weights[:, stimulus])\n",
    "        den = np.sum(np.exp(self.signal_weights[:, stimulus]))\n",
    "        probabilities = num / den\n",
    "        signal = np.random.choice(self.n_signals, p=probabilities)\n",
    "        if signal == self.n_signals-1:\n",
    "            # null action\n",
    "            return -1\n",
    "        self.last_situation = (stimulus, signal)\n",
    "        return signal\n",
    "\n",
    "    def update(self, reward: int) -> None:\n",
    "        # I am capping weight values at 308 due to overflow errors.\n",
    "        stimulus, signal = self.last_situation\n",
    "        self.signal_weights[signal, stimulus] += reward\n",
    "\n",
    "        # after updating the first weight, we must reinforce the surrouding weights\n",
    "        # using a gaussian distribution with a height of 1 and a width of 2\n",
    "        # so that stimulus+2 and stimulus-2 are updated with 1/2 the reward.\n",
    "        for i in range(1, 4):\n",
    "            r = reward * reward_dist(i)\n",
    "\n",
    "            # reward right\n",
    "            if stimulus + i < self.signal_weights.shape[1]:\n",
    "                q_last = self.signal_weights[signal, stimulus + i]\n",
    "                self.signal_weights[signal, stimulus +\n",
    "                                    i] = min(q_last + r, 308)\n",
    "\n",
    "            # reward left\n",
    "            if stimulus - i >= 0:\n",
    "                q_last = self.signal_weights[signal, stimulus - i]\n",
    "                self.signal_weights[signal, stimulus -\n",
    "                                    i] = min(q_last + r, 308)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receiver:\n",
    "    def __init__(self, n_signals, n_actions, q_not: float = 1e-6) -> None:\n",
    "        # n_signals: number of signals that can be sent in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # n_actions: number of actions that can be taken in response,\n",
    "        #            usually equal to the number of states in the world\n",
    "        # q_not:     initial action propensity value\n",
    "        self.n_actions = n_actions\n",
    "        self.action_weights = np.zeros((n_signals, n_actions))\n",
    "        self.action_weights.fill(q_not)\n",
    "        self.last_situation = (0, 0)\n",
    "\n",
    "    def get_action(self, signal: int) -> int:\n",
    "        # exponential calculation\n",
    "        num = np.exp(self.action_weights[signal, :])\n",
    "        den = np.sum(np.exp(self.action_weights[signal, :]))\n",
    "        probabilities = num / den\n",
    "        action = np.random.choice(self.n_actions, p=probabilities)\n",
    "        self.last_situation = (signal, action)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self, reward: int) -> None:\n",
    "        signal, action = self.last_situation\n",
    "        q_last = self.action_weights[signal, action]\n",
    "        self.action_weights[signal, action] = min(q_last + reward, 308)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, epochs, states, signals, actions):\n",
    "        self.send_hist = np.zeros((epochs // 25, signals+1, states))\n",
    "        self.reci_hist = np.zeros((epochs // 25, signals, actions))\n",
    "        self.epochs = epochs\n",
    "        self.ep = 0\n",
    "        # TODO: Genralize functions to work mid-run\n",
    "\n",
    "    def add(self, send_weights, reci_weights):\n",
    "        self.send_hist[self.ep] = send_weights\n",
    "        self.reci_hist[self.ep] = reci_weights\n",
    "        self.ep += 1\n",
    "\n",
    "    def make_gif(self, fps, seed, filename_base): \n",
    "        for i in range(epochs // 25):\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            plt.tight_layout(pad=3)\n",
    "\n",
    "            sns.heatmap(\n",
    "                np.exp(self.send_hist[i]) /\n",
    "                np.exp(self.send_hist[i]).sum(axis=0),\n",
    "                square=True, cbar=False, annot=True, fmt='.1f', ax=axs[0])\n",
    "            axs[0].set_ylabel('messages')\n",
    "            axs[0].set_xlabel('world states')\n",
    "            axs[0].set_title(f'Sender\\'s weights')\n",
    "\n",
    "            sns.heatmap(\n",
    "                np.exp(self.reci_hist[i]) /\n",
    "                np.exp(self.reci_hist[i]).sum(axis=0),\n",
    "                square=True, cbar=False, annot=True, fmt='.2f', ax=axs[1])\n",
    "            axs[1].set_xlabel('actions')\n",
    "            axs[1].set_ylabel('messages')\n",
    "            axs[1].set_title(f'Receiver\\'s weights')\n",
    "            \n",
    "            \n",
    "            fig.suptitle(f'Rollout {i*25}')\n",
    "            plt.savefig(f\"images/game_{i*25}.png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        images = []\n",
    "        for filename in [f'images/game_{j*25}.png' for j in range(epochs // 25)]:\n",
    "            images.append(imageio.imread(filename))\n",
    "        if not os.path.exists(f'gifs/{seed}'):\n",
    "            os.mkdir(f'gifs/{seed}')\n",
    "        imageio.mimsave(f'gifs/{seed}/{filename_base}.gif', images, fps=fps)\n",
    "        # no return\n",
    "\n",
    "    def make_graph(self, seed):\n",
    "        fig, axs = plt.subplots(2, 3, figsize=(10, 8), sharey=True)\n",
    "\n",
    "        ran = range(0, self.epochs, 25)\n",
    "        axs[0, 0].plot(ran, self.send_hist[:, 0, 0], label='action 0')\n",
    "        axs[0, 0].plot(ran, self.send_hist[:, 1, 0], label='action 1')\n",
    "        axs[0, 0].plot(ran, self.send_hist[:, 2, 0], label='null action')\n",
    "        axs[0, 0].set_title('state 0')\n",
    "\n",
    "        axs[0, 1].plot(ran, self.send_hist[:, 0, 5], label='action 0')\n",
    "        axs[0, 1].plot(ran, self.send_hist[:, 1, 5], label='action 1')\n",
    "        axs[0, 1].plot(ran, self.send_hist[:, 2, 5], label='null action')\n",
    "        axs[0, 1].set_title('state 5')\n",
    "\n",
    "        axs[0, 2].plot(ran, self.send_hist[:, 0, 9], label='action 0')\n",
    "        axs[0, 2].plot(ran, self.send_hist[:, 1, 9], label='action 1')\n",
    "        axs[0, 2].plot(ran, self.send_hist[:, 2, 9], label='null action')\n",
    "        axs[0, 2].set_title('state 9')\n",
    "\n",
    "        axs[1, 0].plot(ran, self.send_hist[:, 0, 10], label='action 0')\n",
    "        axs[1, 0].plot(ran, self.send_hist[:, 1, 10], label='action 1')\n",
    "        axs[1, 0].plot(ran, self.send_hist[:, 2, 10], label='null action')\n",
    "        axs[1, 0].set_title('state 10')\n",
    "\n",
    "        axs[1, 1].plot(ran, self.send_hist[:, 0, 14], label='action 0')\n",
    "        axs[1, 1].plot(ran, self.send_hist[:, 1, 14], label='action 1')\n",
    "        axs[1, 1].plot(ran, self.send_hist[:, 2, 14], label='null action')\n",
    "        axs[1, 1].set_title('state 14')\n",
    "\n",
    "        axs[1, 2].plot(ran, self.send_hist[:, 0, 19], label='action 0')\n",
    "        axs[1, 2].plot(ran, self.send_hist[:, 1, 19], label='action 1')\n",
    "        axs[1, 2].plot(ran, self.send_hist[:, 2, 19], label='null action')\n",
    "        axs[1, 2].set_title('state 19')\n",
    "\n",
    "        fig.suptitle(f'Sum action propensities over {self.epochs} epochs')\n",
    "        fig.savefig(f'gifs/{seed}/weights.png')\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipliers = range(1, 6) # 1, 2, 3, 4, 5\n",
    "negatives = [m*(10**x) for m in multipliers for x in range(-3, 1)]\n",
    "weights = [m*(10**x) for m in multipliers for x in range(-6, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "positive = 0.01          # reward for correct action\n",
    "epochs = 20_000          # Number of epochs to train for\n",
    "world_states = 20        # number of world states. evenly split among signals\n",
    "signals = 2              # number of signals sender can send (not including null)\n",
    "actions = 2              # number of actions reciever can respond with\n",
    "gif_fps = 10             # frames per second for gif\n",
    "\n",
    "# world states should be evenly divisible by action and signals\n",
    "assert world_states % signals == world_states % actions == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (negative, weight) in tqdm(enumerate(product(negatives, weights)), total=len(negatives)*len(weights)):\n",
    "    rew = (positive, negative)\n",
    "    seed = int(time.time())\n",
    "    S = Sender(world_states, signals, weight)\n",
    "    R = Receiver(signals, actions, weight)\n",
    "    W = World(world_states, signals, actions, rew, seed)\n",
    "    H = History(epochs, world_states, signals, actions)\n",
    "    past_rewards = slow = 0\n",
    "    for epoch in range(epochs):\n",
    "        stimulus = W.get_state()\n",
    "        signal = S.get_signal(stimulus)\n",
    "        if signal != -1:\n",
    "            action = R.get_action(signal)\n",
    "            reward = W.evaluate(action)\n",
    "            past_rewards += reward\n",
    "            S.update(reward)\n",
    "            R.update(reward)\n",
    "        # else null action\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            # save history\n",
    "            H.add(S.signal_weights, R.action_weights)\n",
    "            \n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            # print(f'Epoch {epoch}, last 100 epochs reward: {past_rewards/100:e}')\n",
    "            slow = past_rewards / 100\n",
    "            past_rewards = 0\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    # now we decide if this is a run to flag\n",
    "    if ((np.argmax(S.signal_weights[:, 9]) == 2 or \n",
    "        np.argmax(S.signal_weights[:, 10]) == 2 or\n",
    "        slow < 0) and (np.argmax(S.signal_weights[:, 5]) != 2 or \n",
    "        np.argmax(S.signal_weights[:, 14]) != 2)):\n",
    "        H.make_gif(gif_fps, seed, f'{world_states}-{actions}-{signals}-game')\n",
    "        H.make_graph(seed)\n",
    "        with open(f'gifs/{seed}/params.txt', 'w') as f:\n",
    "            f.write(f'negative: {negative},\\ninitial weight: {weight}\\nfinal average reward: {slow}')\n",
    "        print(f\"Saved with negative: {negative} and initial weight: {weight}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "This analysis assumes the kernel has been reset, or that History objects are otherwise not available - I will only examine the contents of `params.txt` files left in seed folders in the same directory as this notebook, rather than the specified save location above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = []\n",
    "final_rewards = []\n",
    "negatives = []\n",
    "\n",
    "files = os.listdir(\"./\")\n",
    "\n",
    "for folder in files:\n",
    "    if not os.path.isdir(\"./\" + folder):\n",
    "        continue\n",
    "\n",
    "    with open(\"./\" + folder + \"/params.txt\", \"r\") as f:\n",
    "        params = f.read().splitlines()\n",
    "        initial_weights.append(float(params[1].split(\": \")[1]))\n",
    "        final_rewards.append(float(params[2].split(\": \")[1]))\n",
    "        negatives.append(float(params[0].strip(',').split(\": \")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.02, 0.03, 0.04, 0.05}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3e-06,\n",
       " 4.9999999999999996e-06,\n",
       " 1e-05,\n",
       " 3.0000000000000004e-05,\n",
       " 4e-05,\n",
       " 5e-05,\n",
       " 0.0001,\n",
       " 0.0002,\n",
       " 0.00030000000000000003,\n",
       " 0.0004,\n",
       " 0.0005,\n",
       " 0.002,\n",
       " 0.003,\n",
       " 0.004,\n",
       " 0.005,\n",
       " 0.01,\n",
       " 0.02,\n",
       " 0.03,\n",
       " 0.05,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 10.0,\n",
       " 40.0,\n",
       " 50.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sorted(initial_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards over last 100 epochs\n",
      "Max: 9.50e-03, Seed 1680917024\n",
      "Min: 3.00e-04, Seed 1680927428\n",
      "Difference: 9.20e-03\n"
     ]
    }
   ],
   "source": [
    "print(\"Rewards over last 100 epochs\")\n",
    "print(f\"Max: {max(final_rewards):.2e}, Seed {files[np.argmax(final_rewards)]}\")\n",
    "print(f\"Min: {min(final_rewards):.2e}, Seed {files[np.argmin(final_rewards)]}\")\n",
    "print(f\"Difference: {max(final_rewards) - min(final_rewards):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.17e-03\n"
     ]
    }
   ],
   "source": [
    "print(f\"{np.average(final_rewards):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1680925080', '1680926085', '1680911029', '1680911531',\n",
       "       '1680912522', '1680913006', '1680913981', '1680914954',\n",
       "       '1680915992', '1680917024', '1680917538', '1680918059',\n",
       "       '1680919607', '1680921491', '1680923564'], dtype='<U16')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all seeds scoring between 0.00516 and 0.00517 - about average\n",
    "np.array(files)[list(set(np.where(np.array(final_rewards) > 0.00516)[0]) - set(np.where(np.array(final_rewards) < 0.00517)[0]))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these seeds have a negative score of 0.03 or 0.04. Initial weights range from 3e-6 to 40 for 0.04 and 5e-5 to 5 for 0.03.\n",
    "Given this ratio, I would then expect to see the occurrence of this case broadly in this setup. While close to the desired behavior, all runs highlighted above as close to the average still appear to be trending towards the correct signal in the borderline states, 9 and 10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd92d645a9d62d833baebbf7f83a9be0e163bc5376810cbaf4f96b1078fea88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
